#!/usr/bin/env python3
"""
æµ‹è¯•åŠ¨æ€ç‰¹å¾ç»´åº¦æ¨æ–­åŠŸèƒ½
éªŒè¯PrototypeMemoryèƒ½å¤Ÿæ­£ç¡®å¤„ç†ä¸åŒçš„è¾“å…¥ç‰¹å¾ç»´åº¦
"""

import torch
import torch.nn.functional as F
import logging
from myutils.prototype_separation import PrototypeMemory

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

def test_dynamic_feat_dim():
    """æµ‹è¯•åŠ¨æ€ç‰¹å¾ç»´åº¦æ¨æ–­"""
    print("=" * 60)
    print("æµ‹è¯•åŠ¨æ€ç‰¹å¾ç»´åº¦æ¨æ–­åŠŸèƒ½")
    print("=" * 60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æµ‹è¯•ä¸åŒçš„ç‰¹å¾ç»´åº¦
    test_dims = [16, 32, 64, 128]
    
    for feat_dim in test_dims:
        print(f"\n--- æµ‹è¯•ç‰¹å¾ç»´åº¦: {feat_dim} ---")
        
        # åˆ›å»ºPrototypeMemoryï¼Œä¸æŒ‡å®šfeat_dim
        proto_mem = PrototypeMemory(
            num_classes=2,  # 2ä¸ªå‰æ™¯ç±»
            feat_dim=None,  # å…³é”®ï¼šè®¾ä¸ºNoneï¼Œè¿è¡Œæ—¶æ¨æ–­
            proto_momentum=0.9,
            conf_thresh=0.8,
            lambda_intra=1.0,
            lambda_inter=0.1,
            margin_m=1.0,
            device=device
        ).to(device)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        H, W, D = 8, 8, 8
        num_classes = 3  # åŒ…æ‹¬èƒŒæ™¯
        
        # ç‰¹å¾å¼ é‡ - ä½¿ç”¨å½“å‰æµ‹è¯•çš„ç»´åº¦
        feat = torch.randn(batch_size, feat_dim, H, W, D, device=device, requires_grad=True)
        
        # é¢„æµ‹å¼ é‡
        pred_logits = torch.randn(batch_size, num_classes, H, W, D, device=device, requires_grad=True)
        pred = F.softmax(pred_logits, dim=1)
        
        # æ ‡ç­¾å¼ é‡
        label = torch.randint(0, num_classes, (batch_size, 1, H, W, D), device=device)
        
        # is_labelledæ©ç 
        is_labelled = torch.tensor([True, False], device=device)
        
        print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: {feat.shape}")
        print(f"é¢„æµ‹å½¢çŠ¶: {pred.shape}")
        print(f"æ ‡ç­¾å½¢çŠ¶: {label.shape}")
        
        try:
            # ç¬¬ä¸€æ¬¡forward - åº”è¯¥è‡ªåŠ¨æ¨æ–­ç‰¹å¾ç»´åº¦
            losses = proto_mem(
                feat=feat,
                label=label,
                pred=pred,
                is_labelled=is_labelled,
                epoch_idx=0
            )
            
            print(f"âœ“ æˆåŠŸæ¨æ–­ç‰¹å¾ç»´åº¦: {proto_mem.feat_dim}")
            print(f"âœ“ åŸå‹å½¢çŠ¶: {proto_mem.prototypes.shape}")
            print(f"âœ“ æŸå¤±è®¡ç®—æˆåŠŸ:")
            print(f"  - loss_intra: {losses['intra'].item():.4f}")
            print(f"  - loss_inter: {losses['inter'].item():.4f}")
            print(f"  - loss_total: {losses['total'].item():.4f}")
            print(f"  - n_confident_pixels: {losses['n_confident_pixels']}")
            
            # æµ‹è¯•æ¢¯åº¦åå‘ä¼ æ’­
            total_loss = losses['total']
            total_loss.backward()
            print(f"âœ“ æ¢¯åº¦åå‘ä¼ æ’­æˆåŠŸ")
            
            # æ£€æŸ¥æ¢¯åº¦
            if feat.grad is not None:
                print(f"âœ“ ç‰¹å¾æ¢¯åº¦éé›¶: {feat.grad.abs().sum().item():.6f}")
            
            # ç¬¬äºŒæ¬¡forward - åº”è¯¥ä½¿ç”¨å·²æ¨æ–­çš„ç»´åº¦
            with torch.no_grad():
                feat2 = torch.randn(batch_size, feat_dim, H, W, D, device=device)
                pred2 = F.softmax(torch.randn(batch_size, num_classes, H, W, D, device=device), dim=1)
                
                losses2 = proto_mem(
                    feat=feat2,
                    label=label,
                    pred=pred2,
                    is_labelled=is_labelled,
                    epoch_idx=1
                )
                print(f"âœ“ ç¬¬äºŒæ¬¡forwardæˆåŠŸ: {losses2['total'].item():.4f}")
            
        except Exception as e:
            print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
            continue
    
    print("\n" + "=" * 60)
    print("åŠ¨æ€ç‰¹å¾ç»´åº¦æ¨æ–­æµ‹è¯•å®Œæˆ")
    print("=" * 60)

def test_dimension_mismatch_error():
    """æµ‹è¯•ç»´åº¦ä¸åŒ¹é…æ—¶çš„é”™è¯¯å¤„ç†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»´åº¦ä¸åŒ¹é…é”™è¯¯å¤„ç†")
    print("=" * 60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åˆ›å»ºPrototypeMemoryï¼ŒæŒ‡å®šç‰¹å®šçš„feat_dim
    proto_mem = PrototypeMemory(
        num_classes=2,
        feat_dim=64,  # æŒ‡å®šä¸º64
        proto_momentum=0.9,
        conf_thresh=0.8,
        device=device
    ).to(device)
    
    # åˆ›å»ºä¸åŒ¹é…ç»´åº¦çš„è¾“å…¥
    batch_size = 2
    H, W, D = 8, 8, 8
    wrong_feat_dim = 32  # ä¸æŒ‡å®šçš„64ä¸åŒ¹é…
    
    feat = torch.randn(batch_size, wrong_feat_dim, H, W, D, device=device)
    pred = F.softmax(torch.randn(batch_size, 3, H, W, D, device=device), dim=1)
    label = torch.randint(0, 3, (batch_size, 1, H, W, D), device=device)
    is_labelled = torch.tensor([True, False], device=device)
    
    print(f"åŸå‹æœŸæœ›ç»´åº¦: {proto_mem.feat_dim}")
    print(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {wrong_feat_dim}")
    
    try:
        losses = proto_mem(
            feat=feat,
            label=label,
            pred=pred,
            is_labelled=is_labelled,
            epoch_idx=0
        )
        print("âœ— é¢„æœŸçš„ç»´åº¦ä¸åŒ¹é…é”™è¯¯æ²¡æœ‰å‘ç”Ÿ")
    except RuntimeError as e:
        if "ç‰¹å¾ç»´åº¦ä¸åŒ¹é…" in str(e):
            print(f"âœ“ æ­£ç¡®æ•è·ç»´åº¦ä¸åŒ¹é…é”™è¯¯: {e}")
        else:
            print(f"âœ— æ„å¤–çš„RuntimeError: {e}")
    except Exception as e:
        print(f"âœ— æ„å¤–çš„é”™è¯¯ç±»å‹: {e}")

def test_integration_simulation():
    """æ¨¡æ‹Ÿå®é™…è®­ç»ƒé›†æˆåœºæ™¯"""
    print("\n" + "=" * 60)
    print("æ¨¡æ‹Ÿå®é™…è®­ç»ƒé›†æˆåœºæ™¯")
    print("=" * 60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # æ¨¡æ‹ŸLAæ•°æ®é›†é…ç½®
    proto_mem = PrototypeMemory(
        num_classes=1,  # LAæ•°æ®é›†ï¼š1ä¸ªå‰æ™¯ç±»
        feat_dim=None,  # è¿è¡Œæ—¶æ¨æ–­
        proto_momentum=0.95,
        conf_thresh=0.85,
        lambda_intra=0.3,
        lambda_inter=0.1,
        margin_m=1.5,
        device=device
    ).to(device)
    
    # æ¨¡æ‹ŸVNetè¾“å‡ºç‰¹å¾ï¼ˆé€šå¸¸æ˜¯16ç»´ï¼‰
    actual_feat_dim = 16  # VNet decoderè¾“å‡ºç»´åº¦
    batch_size = 4
    H, W, D = 16, 16, 16
    
    print(f"æ¨¡æ‹ŸVNetç‰¹å¾ç»´åº¦: {actual_feat_dim}")
    
    # æ¨¡æ‹Ÿå¤šä¸ªè®­ç»ƒæ­¥éª¤
    for step in range(5):
        print(f"\n--- è®­ç»ƒæ­¥éª¤ {step + 1} ---")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        feat = torch.randn(batch_size, actual_feat_dim, H, W, D, device=device, requires_grad=True)
        pred_logits = torch.randn(batch_size, 2, H, W, D, device=device, requires_grad=True)  # 2ç±»ï¼šèƒŒæ™¯+å‰æ™¯
        pred = F.softmax(pred_logits, dim=1)
        label = torch.randint(0, 2, (batch_size, 1, H, W, D), device=device)
        is_labelled = torch.tensor([True, True, False, False], device=device)  # å‰2ä¸ªæœ‰æ ‡ç­¾
        
        try:
            # æŸå¤±è®¡ç®—é˜¶æ®µï¼ˆä¸æ›´æ–°åŸå‹ï¼‰
            losses = proto_mem(
                feat=feat,
                label=label,
                pred=pred,
                is_labelled=is_labelled,
                epoch_idx=None  # ä¸æ›´æ–°åŸå‹
            )
            
            # æ¨¡æ‹Ÿæ€»æŸå¤±å’Œåå‘ä¼ æ’­
            supervised_loss = torch.randn(1, device=device, requires_grad=True)
            consistency_loss = torch.randn(1, device=device, requires_grad=True)
            
            total_loss = supervised_loss + consistency_loss + 0.3 * losses['total']
            total_loss.backward()
            
            print(f"âœ“ æŸå¤±è®¡ç®—æˆåŠŸ: {losses['total'].item():.4f}")
            print(f"  - ç½®ä¿¡åƒç´ æ•°: {losses['n_confident_pixels']}")
            print(f"  - å·²åˆå§‹åŒ–åŸå‹æ•°: {losses['n_initialized_protos']}")
            
            # æ¨¡æ‹Ÿoptimizer.step()åçš„åŸå‹æ›´æ–°
            if step % 2 == 0:  # æ¯2æ­¥æ›´æ–°ä¸€æ¬¡åŸå‹
                with torch.no_grad():
                    update_feat = feat.detach().clone()
                    update_pred = pred.detach().clone()
                    
                    _ = proto_mem(
                        feat=update_feat,
                        label=label,
                        pred=update_pred,
                        is_labelled=is_labelled,
                        epoch_idx=step
                    )
                    print(f"âœ“ åŸå‹æ›´æ–°å®Œæˆ")
                    
                    # è·å–ç»Ÿè®¡ä¿¡æ¯
                    stats = proto_mem.get_prototype_statistics()
                    print(f"  - åŸå‹ç»Ÿè®¡: {stats['num_initialized']}/{stats['total_classes']} å·²åˆå§‹åŒ–")
            
        except Exception as e:
            print(f"âœ— æ­¥éª¤ {step + 1} å¤±è´¥: {e}")
            break
    
    print(f"\nâœ“ é›†æˆæ¨¡æ‹Ÿæµ‹è¯•å®Œæˆ")
    print(f"æœ€ç»ˆåŸå‹ç»´åº¦: {proto_mem.feat_dim}")

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_dynamic_feat_dim()
    test_dimension_mismatch_error()
    test_integration_simulation()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼") 
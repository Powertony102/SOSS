#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬ï¼šåœ¨train_cov_dfp_3d.pyä¸­é›†æˆPrototypeMemoryçš„ç¤ºä¾‹
å®Œå…¨è§£å†³autogradå›¾äº¤å‰å¼•ç”¨é—®é¢˜
"""

import torch
from myutils.prototype_separation import PrototypeMemory


def train_epoch_with_prototype_separation_fixed(
    model, 
    train_loader, 
    optimizer, 
    proto_memory, 
    current_epoch, 
    args
):
    """
    ä¿®å¤ç‰ˆæœ¬çš„è®­ç»ƒepochå‡½æ•°ï¼Œé›†æˆåŸå‹åˆ†ç¦»æŸå¤±
    
    Args:
        model: è®­ç»ƒæ¨¡å‹
        train_loader: æ•°æ®åŠ è½½å™¨
        optimizer: ä¼˜åŒ–å™¨
        proto_memory: PrototypeMemoryå®ä¾‹
        current_epoch: å½“å‰epoch
        args: è®­ç»ƒå‚æ•°
    """
    model.train()
    
    for batch_idx, (sampled_batch, is_labelled_mask) in enumerate(train_loader):
        # æå–æ•°æ®
        volume_batch = sampled_batch['image']
        label_batch = sampled_batch['label']
        
        # **ç¬¬1æ­¥**: æ¨¡å‹å‰å‘ä¼ æ’­
        outputs_v, outputs_a, embedding_v, embedding_a = model(volume_batch)
        
        # **ç¬¬2æ­¥**: è®¡ç®—åŸºç¡€æŸå¤±
        dice_loss = compute_dice_loss(outputs_v, label_batch)
        consistency_loss = compute_consistency_loss(outputs_v, outputs_a)
        hcc_loss = compute_hcc_loss(embedding_v, embedding_a)
        
        # **ç¬¬3æ­¥**: åŸå‹åˆ†ç¦»æŸå¤± - å…³é”®ä¿®å¤ç‚¹
        # ä½¿ç”¨embedding_vä½œä¸ºç‰¹å¾ï¼Œç¡®ä¿å…¶requires_grad=True
        decoder_features = embedding_v  # (B, C, H, W, D)
        predictions = torch.softmax(outputs_v, dim=1)  # (B, K, H, W, D)
        
        # è®¡ç®—åŸå‹æŸå¤±ï¼ˆä¸æ›´æ–°åŸå‹ï¼Œé¿å…autogradå›¾é—®é¢˜ï¼‰
        proto_losses = proto_memory(
            feat=decoder_features,
            label=label_batch,
            pred=predictions,
            is_labelled=is_labelled_mask,
            epoch_idx=None  # å…³é”®ï¼šè®¾ä¸ºNoneé¿å…åŸå‹æ›´æ–°æ—¶çš„æ¢¯åº¦é—®é¢˜
        )
        
        # **ç¬¬4æ­¥**: ç»„åˆæ€»æŸå¤±
        total_loss = (
            args.lamda * dice_loss + 
            args.lambda_consistency * consistency_loss + 
            args.lambda_hcc * hcc_loss +
            args.lambda_prototype * proto_losses['total']  # æ·»åŠ åŸå‹æŸå¤±
        )
        
        # **ç¬¬5æ­¥**: åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # **ç¬¬6æ­¥**: åŸå‹æ›´æ–° - åœ¨optimizer.step()ä¹‹åç‹¬ç«‹è¿›è¡Œ
        if batch_idx % args.prototype_update_interval == 0:
            with torch.no_grad():
                # åˆ›å»ºæ— æ¢¯åº¦çš„ç‰¹å¾å‰¯æœ¬ç”¨äºåŸå‹æ›´æ–°
                update_features = embedding_v.detach().clone()
                update_predictions = predictions.detach().clone()
                
                # ç‹¬ç«‹æ›´æ–°åŸå‹
                _ = proto_memory(
                    feat=update_features,
                    label=label_batch,
                    pred=update_predictions,
                    is_labelled=is_labelled_mask,
                    epoch_idx=current_epoch
                )
                
                # æ¸…ç†ä¸´æ—¶å¼ é‡
                del update_features, update_predictions
        
        # **ç¬¬7æ­¥**: æ—¥å¿—è®°å½•ï¼ˆå®‰å…¨åœ°æå–æŸå¤±å€¼ï¼‰
        if batch_idx % args.log_interval == 0:
            dice_val = dice_loss.detach().item()
            consistency_val = consistency_loss.detach().item()
            hcc_val = hcc_loss.detach().item()
            proto_intra_val = proto_losses['intra'].detach().item()
            proto_inter_val = proto_losses['inter'].detach().item()
            proto_total_val = proto_losses['total'].detach().item()
            total_val = total_loss.detach().item()
            
            print(f"Epoch {current_epoch}, Batch {batch_idx}:")
            print(f"  Dice: {dice_val:.4f}")
            print(f"  Consistency: {consistency_val:.4f}")
            print(f"  HCC: {hcc_val:.4f}")
            print(f"  Proto-Intra: {proto_intra_val:.4f}")
            print(f"  Proto-Inter: {proto_inter_val:.4f}")
            print(f"  Proto-Total: {proto_total_val:.4f}")
            print(f"  Total: {total_val:.4f}")


def initialize_prototype_memory_for_la_dataset(device):
    """ä¸ºLAæ•°æ®é›†åˆå§‹åŒ–PrototypeMemory"""
    return PrototypeMemory(
        num_classes=1,          # LAæ•°æ®é›†ï¼š1ä¸ªå‰æ™¯ç±»ï¼ˆå¿ƒæˆ¿ï¼‰+ 1ä¸ªèƒŒæ™¯ç±»
        feat_dim=64,            # æ ¹æ®ä½ çš„embedding_dimè®¾ç½®
        proto_momentum=0.95,    # é«˜åŠ¨é‡ç¡®ä¿ç¨³å®šæ›´æ–°
        conf_thresh=0.85,       # é«˜ç½®ä¿¡åº¦é˜ˆå€¼
        lambda_intra=0.3,       # ç±»å†…ç´§è‡´æ€§æƒé‡
        lambda_inter=0.1,       # ç±»é—´åˆ†ç¦»æƒé‡
        margin_m=1.5,           # åˆ†ç¦»è¾¹ç•Œ
        device=device
    ).to(device)


def main_training_example():
    """å®Œæ•´çš„è®­ç»ƒç¤ºä¾‹"""
    print("LAæ•°æ®é›†è®­ç»ƒç¤ºä¾‹ - é›†æˆåŸå‹åˆ†ç¦»æ¨¡å—")
    print("="*60)
    
    # è®¾å¤‡è®¾ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–åŸå‹å†…å­˜
    proto_memory = initialize_prototype_memory_for_la_dataset(device)
    print("âœ“ PrototypeMemoryåˆå§‹åŒ–å®Œæˆ")
    
    # æ¨¡æ‹Ÿè®­ç»ƒå‚æ•°
    class Args:
        lamda = 0.5
        lambda_consistency = 0.1
        lambda_hcc = 0.3
        lambda_prototype = 0.3        # åŸå‹æŸå¤±æƒé‡
        prototype_update_interval = 5  # æ¯5ä¸ªbatchæ›´æ–°ä¸€æ¬¡åŸå‹
        log_interval = 10
    
    args = Args()
    
    print(f"\nè®­ç»ƒå‚æ•°:")
    print(f"  DiceæŸå¤±æƒé‡: {args.lamda}")
    print(f"  ä¸€è‡´æ€§æŸå¤±æƒé‡: {args.lambda_consistency}")
    print(f"  HCCæŸå¤±æƒé‡: {args.lambda_hcc}")
    print(f"  åŸå‹æŸå¤±æƒé‡: {args.lambda_prototype}")
    print(f"  åŸå‹æ›´æ–°é—´éš”: {args.prototype_update_interval} batches")
    
    print(f"\né›†æˆä»£ç ç¤ºä¾‹:")
    print("```python")
    print("# åœ¨train_cov_dfp_3d.pyçš„ä¸»è®­ç»ƒå¾ªç¯ä¸­:")
    print("proto_memory = initialize_prototype_memory_for_la_dataset(device)")
    print("")
    print("for epoch in range(max_epochs):")
    print("    train_epoch_with_prototype_separation_fixed(")
    print("        model=model,")
    print("        train_loader=train_loader,")
    print("        optimizer=optimizer,")
    print("        proto_memory=proto_memory,")
    print("        current_epoch=epoch,")
    print("        args=args")
    print("    )")
    print("```")
    
    print(f"\nå…³é”®ä¿®å¤ç‚¹:")
    print("1. âœ“ åŸå‹æ›´æ–°å’ŒæŸå¤±è®¡ç®—å®Œå…¨åˆ†ç¦»")
    print("2. âœ“ ä½¿ç”¨detach().clone()åˆ›å»ºæ— æ¢¯åº¦å‰¯æœ¬")
    print("3. âœ“ åœ¨optimizer.step()åç‹¬ç«‹æ›´æ–°åŸå‹")
    print("4. âœ“ æ‰€æœ‰æ‰“å°ä½¿ç”¨.detach().item()æå–å€¼")
    print("5. âœ“ åŠæ—¶æ¸…ç†ä¸´æ—¶å¼ é‡é¿å…å†…å­˜æ³„æ¼")
    
    return proto_memory


# è¾…åŠ©å‡½æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
def compute_dice_loss(outputs, labels):
    """æ¨¡æ‹ŸDiceæŸå¤±è®¡ç®—"""
    return torch.randn(1, requires_grad=True)

def compute_consistency_loss(outputs_v, outputs_a):
    """æ¨¡æ‹Ÿä¸€è‡´æ€§æŸå¤±è®¡ç®—"""
    return torch.randn(1, requires_grad=True)

def compute_hcc_loss(embedding_v, embedding_a):
    """æ¨¡æ‹ŸHCCæŸå¤±è®¡ç®—"""
    return torch.randn(1, requires_grad=True)


if __name__ == "__main__":
    print("å®Œå…¨ä¿®å¤çš„PrototypeMemoryé›†æˆç¤ºä¾‹")
    print("="*60)
    
    try:
        proto_memory = main_training_example()
        
        print(f"\nâœ… é›†æˆç¤ºä¾‹åˆ›å»ºæˆåŠŸï¼")
        print(f"ğŸ’¡ è¯·å°†ä¸Šè¿°ä»£ç é›†æˆåˆ°æ‚¨çš„train_cov_dfp_3d.pyä¸­")
        print(f"ğŸ”§ è®°ä½è®¾ç½®args.lambda_prototype=0.3ä½œä¸ºåŸå‹æŸå¤±æƒé‡")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 
#!/usr/bin/env python3
"""
æµ‹è¯•åŸºäºSS-Netçš„å¯¹æ¯”å­¦ä¹ åŸå‹ç®¡ç†å™¨
éªŒè¯å¯¹æ¯”å­¦ä¹ æŸå¤±è®¡ç®—çš„æ­£ç¡®æ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from myutils.contrastive_prototype_manager import ContrastivePrototypeManager

def test_contrastive_prototype_manager():
    """æµ‹è¯•å¯¹æ¯”å­¦ä¹ åŸå‹ç®¡ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•åŸºäºSS-Netçš„å¯¹æ¯”å­¦ä¹ åŸå‹ç®¡ç†å™¨...")
    
    # è®¾ç½®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å‚æ•°
    num_classes = 3  # èƒŒæ™¯ + 2ä¸ªå‰æ™¯ç±»
    feature_dim = 256  # æŒ‰ç…§SS-Netçš„ç‰¹å¾ç»´åº¦
    elements_per_class = 16  # æµ‹è¯•ç”¨è¾ƒå°æ•°å€¼
    batch_size = 2
    H, W, D = 32, 32, 32
    
    # åˆ›å»ºç®¡ç†å™¨ - ä¸ä½¿ç”¨å­¦ä¹ çš„é€‰æ‹©å™¨ç‰ˆæœ¬
    print("\nğŸ“‹ æµ‹è¯•1: ç®€å•ç‰ˆæœ¬ï¼ˆä¸ä½¿ç”¨å­¦ä¹ çš„é€‰æ‹©å™¨ï¼‰")
    simple_manager = ContrastivePrototypeManager(
        num_classes=num_classes,
        feature_dim=feature_dim,
        elements_per_class=elements_per_class,
        confidence_threshold=0.8,
        use_learned_selector=False,
        device=device
    )
    
    # æ¨¡æ‹Ÿæ•°æ®
    features = torch.randn(batch_size, feature_dim, H, W, D, device=device, requires_grad=True)
    predictions = torch.randn(batch_size, num_classes, H, W, D, device=device)
    labels = torch.randint(0, num_classes, (batch_size, H, W, D), device=device)
    
    print(f"âœ… åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æˆåŠŸ - ç‰¹å¾å½¢çŠ¶: {features.shape}")
    
    try:
        # ç¬¬ä¸€æ¬¡æ›´æ–°å’ŒæŸå¤±è®¡ç®—
        total_loss, loss_dict = simple_manager.update_and_compute_loss(
            features=features,
            predictions=predictions,
            labels=labels,
            is_labeled=True,
            contrastive_weight=1.0,
            intra_weight=0.1,
            inter_weight=0.1
        )
        
        print(f"âœ… ç¬¬ä¸€æ¬¡æŸå¤±è®¡ç®—æˆåŠŸ:")
        for key, value in loss_dict.items():
            print(f"   {key}: {value:.4f}")
            
        # æ£€æŸ¥å†…å­˜çŠ¶æ€
        memory_info = simple_manager.get_memory_info()
        print(f"âœ… å†…å­˜çŠ¶æ€: {memory_info['memory_status']}")
        
        # ç¬¬äºŒæ¬¡æ›´æ–°ï¼ˆåº”è¯¥æœ‰å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼‰
        new_features = torch.randn(batch_size, feature_dim, H, W, D, device=device, requires_grad=True)
        new_predictions = torch.randn(batch_size, num_classes, H, W, D, device=device)
        new_labels = torch.randint(0, num_classes, (batch_size, H, W, D), device=device)
        
        total_loss2, loss_dict2 = simple_manager.update_and_compute_loss(
            features=new_features,
            predictions=new_predictions,
            labels=new_labels,
            is_labeled=True,
            contrastive_weight=1.0,
            intra_weight=0.1,
            inter_weight=0.1
        )
        
        print(f"âœ… ç¬¬äºŒæ¬¡æŸå¤±è®¡ç®—æˆåŠŸ:")
        for key, value in loss_dict2.items():
            print(f"   {key}: {value:.4f}")
            
        # æ£€æŸ¥æ¢¯åº¦
        if total_loss2.requires_grad:
            total_loss2.backward()
            if new_features.grad is not None:
                grad_norm = torch.norm(new_features.grad).item()
                print(f"âœ… æ¢¯åº¦åå‘ä¼ æ’­æˆåŠŸ - æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}")
            else:
                print("âŒ æœªæ£€æµ‹åˆ°æ¢¯åº¦")
                return False
        
    except Exception as e:
        print(f"âŒ ç®€å•ç‰ˆæœ¬æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•å­¦ä¹ çš„é€‰æ‹©å™¨ç‰ˆæœ¬
    print("\nğŸ“‹ æµ‹è¯•2: å­¦ä¹ çš„é€‰æ‹©å™¨ç‰ˆæœ¬")
    learned_manager = ContrastivePrototypeManager(
        num_classes=num_classes,
        feature_dim=feature_dim,
        elements_per_class=elements_per_class,
        confidence_threshold=0.8,
        use_learned_selector=True,
        device=device
    )
    
    try:
        # é‡ç½®æ¢¯åº¦
        if features.grad is not None:
            features.grad.zero_()
        
        # ä½¿ç”¨å­¦ä¹ çš„é€‰æ‹©å™¨è®¡ç®—æŸå¤±
        features_learned = torch.randn(batch_size, feature_dim, H, W, D, device=device, requires_grad=True)
        predictions_learned = torch.randn(batch_size, num_classes, H, W, D, device=device)
        labels_learned = torch.randint(0, num_classes, (batch_size, H, W, D), device=device)
        
        # ç¬¬ä¸€æ¬¡æ›´æ–°
        loss1, loss_dict1 = learned_manager.update_and_compute_loss(
            features=features_learned,
            predictions=predictions_learned,
            labels=labels_learned,
            is_labeled=True
        )
        
        # ç¬¬äºŒæ¬¡æ›´æ–°ï¼ˆåº”è¯¥æœ‰å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼‰
        features_learned2 = torch.randn(batch_size, feature_dim, H, W, D, device=device, requires_grad=True)
        predictions_learned2 = torch.randn(batch_size, num_classes, H, W, D, device=device)
        labels_learned2 = torch.randint(0, num_classes, (batch_size, H, W, D), device=device)
        
        loss2, loss_dict2 = learned_manager.update_and_compute_loss(
            features=features_learned2,
            predictions=predictions_learned2,
            labels=labels_learned2,
            is_labeled=True
        )
        
        print(f"âœ… å­¦ä¹ çš„é€‰æ‹©å™¨ç‰ˆæœ¬æŸå¤±è®¡ç®—æˆåŠŸ:")
        for key, value in loss_dict2.items():
            print(f"   {key}: {value:.4f}")
            
        # æ£€æŸ¥é€‰æ‹©å™¨æ¨¡å—
        feature_selectors, memory_selectors = learned_manager.get_selectors()
        if feature_selectors is not None and memory_selectors is not None:
            print(f"âœ… é€‰æ‹©å™¨æ¨¡å—æ•°é‡: ç‰¹å¾é€‰æ‹©å™¨={len(feature_selectors)}, å†…å­˜é€‰æ‹©å™¨={len(memory_selectors)}")
        
        # æ¢¯åº¦æ£€æŸ¥
        loss2.backward()
        if features_learned2.grad is not None:
            grad_norm = torch.norm(features_learned2.grad).item()
            print(f"âœ… å­¦ä¹ ç‰ˆæœ¬æ¢¯åº¦åå‘ä¼ æ’­æˆåŠŸ - æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}")
        
    except Exception as e:
        print(f"âŒ å­¦ä¹ çš„é€‰æ‹©å™¨ç‰ˆæœ¬æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\nğŸ“‹ æµ‹è¯•3: æ— æ ‡ç­¾æ•°æ®æµ‹è¯•")
    try:
        # æ— æ ‡ç­¾æ•°æ®æµ‹è¯•
        unlabeled_features = torch.randn(batch_size, feature_dim, H, W, D, device=device, requires_grad=True)
        unlabeled_predictions = torch.randn(batch_size, num_classes, H, W, D, device=device)
        
        # åˆ›å»ºé«˜ç½®ä¿¡åº¦é¢„æµ‹
        unlabeled_predictions[:, 1, :, :, :] = 5.0  # é«˜ç½®ä¿¡åº¦é¢„æµ‹ä¸ºç±»åˆ«1
        
        loss_unlabeled, loss_dict_unlabeled = simple_manager.update_and_compute_loss(
            features=unlabeled_features,
            predictions=unlabeled_predictions,
            labels=None,
            is_labeled=False
        )
        
        print(f"âœ… æ— æ ‡ç­¾æ•°æ®æµ‹è¯•æˆåŠŸ:")
        for key, value in loss_dict_unlabeled.items():
            print(f"   {key}: {value:.4f}")
            
    except Exception as e:
        print(f"âŒ æ— æ ‡ç­¾æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯¹æ¯”å­¦ä¹ åŸå‹ç®¡ç†å™¨å·¥ä½œæ­£å¸¸ã€‚")
    return True

def test_contrastive_loss_computation():
    """å•ç‹¬æµ‹è¯•å¯¹æ¯”å­¦ä¹ æŸå¤±è®¡ç®—"""
    print("\nğŸ” è¯¦ç»†æµ‹è¯•å¯¹æ¯”å­¦ä¹ æŸå¤±è®¡ç®—...")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    num_classes = 2  # ç®€åŒ–ä¸º2ç±»
    feature_dim = 64
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = ContrastivePrototypeManager(
        num_classes=num_classes,
        feature_dim=feature_dim,
        elements_per_class=4,  # å°æ•°é‡ä¾¿äºè§‚å¯Ÿ
        confidence_threshold=0.5,
        use_learned_selector=False,
        device=device
    )
    
    # æ‰‹åŠ¨åˆ›å»ºå†…å­˜
    for class_id in range(num_classes):
        # ä¸ºæ¯ä¸ªç±»åˆ›å»ºä¸åŒçš„ç‰¹å¾åˆ†å¸ƒ
        class_features = torch.randn(4, feature_dim, device=device) + class_id * 2.0
        manager.memory[class_id] = class_features.cpu().numpy()
    
    manager.initialized = True
    
    # åˆ›å»ºæµ‹è¯•ç‰¹å¾
    test_features = torch.randn(8, feature_dim, device=device, requires_grad=True)
    test_labels = torch.tensor([0, 0, 1, 1, 0, 1, 1, 0], device=device)
    
    # è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
    contrastive_loss = manager.contrastive_class_to_class_learned_memory(test_features, test_labels)
    
    print(f"âœ… å¯¹æ¯”å­¦ä¹ æŸå¤±: {contrastive_loss.item():.4f}")
    
    # æ£€æŸ¥æŸå¤±çš„åˆç†æ€§
    if contrastive_loss.item() > 0:
        print("âœ… æŸå¤±å€¼åˆç†ï¼ˆå¤§äº0ï¼‰")
    else:
        print("âš ï¸ æŸå¤±å€¼ä¸º0ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥")
    
    # æ¢¯åº¦æ£€æŸ¥
    contrastive_loss.backward()
    if test_features.grad is not None:
        grad_norm = torch.norm(test_features.grad).item()
        print(f"âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ - æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åŸºäºSS-Netçš„å¯¹æ¯”å­¦ä¹ åŸå‹ç®¡ç†å™¨æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    # è¿è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•
    success1 = test_contrastive_prototype_manager()
    
    if success1:
        # è¿è¡Œè¯¦ç»†çš„æŸå¤±è®¡ç®—æµ‹è¯•
        success2 = test_contrastive_loss_computation()
        
        if success2:
            print("\nğŸ’¡ å®ç°è¦ç‚¹:")
            print("1. ä¸¥æ ¼æŒ‰ç…§SS-Netçš„å¯¹æ¯”å­¦ä¹ æŸå¤±è®¡ç®—æ–¹å¼")
            print("2. ä½¿ç”¨L2å½’ä¸€åŒ–å’Œç›¸ä¼¼æ€§çŸ©é˜µè®¡ç®—")
            print("3. æ”¯æŒå­¦ä¹ çš„ç‰¹å¾é€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰")
            print("4. æ­£ç¡®çš„æ¢¯åº¦ç®¡ç†å’Œå†…å­˜æ›´æ–°")
            print("5. ä¸åŸæœ‰æ¡†æ¶å…¼å®¹çš„æ¥å£è®¾è®¡")
            
            print("\nğŸ¯ ä½¿ç”¨å»ºè®®:")
            print("- å¯¹äºåˆæœŸè®­ç»ƒï¼Œä½¿ç”¨ç®€å•ç‰ˆæœ¬ (use_learned_selector=False)")
            print("- å¯¹äºé«˜çº§åº”ç”¨ï¼Œå¯ä»¥å¯ç”¨å­¦ä¹ çš„é€‰æ‹©å™¨")
            print("- è°ƒæ•´ contrastive_weight æ¥å¹³è¡¡å¯¹æ¯”å­¦ä¹ å’Œä¼ ç»ŸæŸå¤±")
            print("- ç›‘æ§å¯¹æ¯”å­¦ä¹ æŸå¤±çš„å˜åŒ–è¶‹åŠ¿")
    
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")

if __name__ == "__main__":
    main() 